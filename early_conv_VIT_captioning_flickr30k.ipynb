{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f440303e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:02.109557Z",
     "start_time": "2021-07-17T09:27:58.644157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/17/21 09:28:00] </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f\">WARNING </span> Skipping cv2 import                        <a href=\"file:///opt/conda/lib/python3.8/site-packages/torch_snippets/loader.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">loader.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:&lt;module&gt;:48</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/17/21 09:28:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;31mWARNING \u001b[0m Skipping cv2 import                        \u001b]8;id=1626514080.6579294-116392;file:///opt/conda/lib/python3.8/site-packages/torch_snippets/loader.py\u001b\\\u001b[2mloader.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:<module>:48\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/17/21 09:28:01] </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f\">WARNING </span> sklearn is not found. Skipping relevant  <a href=\"file:///opt/conda/lib/python3.8/site-packages/torch_snippets/__init__.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:&lt;module&gt;:13</span>\n",
       "                             imports from submodule `sklegos`                                \n",
       "                             Exception: No module named <span style=\"color: #008000; text-decoration-color: #008000\">'sklego'</span>                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/17/21 09:28:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;31mWARNING \u001b[0m sklearn is not found. Skipping relevant  \u001b]8;id=1626514081.88263-748117;file:///opt/conda/lib/python3.8/site-packages/torch_snippets/__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:<module>:13\u001b[0m\n",
       "                             imports from submodule `sklegos`                                \n",
       "                             Exception: No module named \u001b[32m'sklego'\u001b[0m                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# credits for transformer backbone with multi-query attention: https://github.com/M-e-r-c-u-r-y/\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler  #mixed prec\n",
    "from torch_snippets import *\n",
    "from torchtext.vocab import Vocab\n",
    "from torchsummary import summary\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess as sub\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize, AffineTransform, warp\n",
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "from scipy import spatial\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import functools as fts\n",
    "import warnings\n",
    "from collections import Counter, deque\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69213a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:02.117167Z",
     "start_time": "2021-07-17T09:28:02.111573Z"
    }
   },
   "outputs": [],
   "source": [
    "ic.configureOutput(includeContext=True)\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89243549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:02.126341Z",
     "start_time": "2021-07-17T09:28:02.119365Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b5d1bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:02.485543Z",
     "start_time": "2021-07-17T09:28:02.128383Z"
    }
   },
   "outputs": [],
   "source": [
    "savename = 'pic2sec_pytorch_vit_30k_glove_convemb'\n",
    "this_dir = os.getcwd()\n",
    "data_dir = os.path.join(this_dir, '30k', 'flickr30k_images')\n",
    "pic_dir = os.path.join(this_dir, '30k', 'flickr30k_images', 'flickr30k_images')\n",
    "ckp_dir = os.path.join(this_dir, 'ckp')\n",
    "df_source = pd.read_csv(os.path.join(data_dir, 'results.csv'), sep='|')  # .sample(frac=0.1)\n",
    "df_source.dropna(inplace=True)\n",
    "df_source = df_source[df_source['caption'].str.len()>10]  # I consider shorter sentences as an impaired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c1267b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:02.505285Z",
     "start_time": "2021-07-17T09:28:02.487524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158910</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A man in shorts and a Hawaiian shirt leans ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158911</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>A young man hanging over the side of a boat ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158912</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>A man is leaning off of the side of a blue an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158913</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man riding a small boat in a harbor , with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158914</th>\n",
       "      <td>998845445.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>A man on a moored blue and white boat with hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158911 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image comment_number  \\\n",
       "0       1000092795.jpg              0   \n",
       "1       1000092795.jpg              1   \n",
       "2       1000092795.jpg              2   \n",
       "3       1000092795.jpg              3   \n",
       "4       1000092795.jpg              4   \n",
       "...                ...            ...   \n",
       "158910   998845445.jpg              0   \n",
       "158911   998845445.jpg              1   \n",
       "158912   998845445.jpg              2   \n",
       "158913   998845445.jpg              3   \n",
       "158914   998845445.jpg              4   \n",
       "\n",
       "                                                  caption  \n",
       "0        Two young guys with shaggy hair look at their...  \n",
       "1        Two young , White males are outside near many...  \n",
       "2        Two men in green shirts are standing in a yard .  \n",
       "3            A man in a blue shirt standing in a garden .  \n",
       "4                 Two friends enjoy time spent together .  \n",
       "...                                                   ...  \n",
       "158910   A man in shorts and a Hawaiian shirt leans ov...  \n",
       "158911   A young man hanging over the side of a boat ,...  \n",
       "158912   A man is leaning off of the side of a blue an...  \n",
       "158913   A man riding a small boat in a harbor , with ...  \n",
       "158914   A man on a moored blue and white boat with hi...  \n",
       "\n",
       "[158911 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b1fe355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:02.528072Z",
     "start_time": "2021-07-17T09:28:02.506965Z"
    }
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    'Class acts as a metadata container'\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.words = {'<sos>', '<eos>'}\n",
    "        self.word2count = {'<sos>': 0, '<eos>': 0,}\n",
    "        self.maxlen = 0\n",
    "        self.stats = np.array([], dtype=np.int_)  # sentence length stats\n",
    "\n",
    "    def add_sentence(self, sentence: list):\n",
    "        self.stats = np.append(self.stats,\n",
    "                               len(sentence))\n",
    "        if len(sentence) > self.maxlen:\n",
    "            self.maxlen = len(sentence) \n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "        self.word2count['<sos>'] += 1\n",
    "        self.word2count['<eos>'] += 1\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        if word not in self.words:\n",
    "            self.words.add(word)\n",
    "            self.word2count[word] = 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def plt_hist(self, show=True):\n",
    "        plot = plt.hist(self.stats, bins='fd')\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            return plot\n",
    "    \n",
    "    def n_words(self):\n",
    "        return len(self.words) - 2 # without <sos> <eos>\n",
    "\n",
    "\n",
    "# to trzeba dokończyć\n",
    "\n",
    "class Tokenizer:\n",
    "    'Class for handling text<->indexes operations'\n",
    "    \n",
    "    def __init__(self, voc: torchtext.vocab.Vocab=None,\n",
    "                 tokenizer='basic_english',\n",
    "                 tok_lang='en'):\n",
    "        self.voc = voc\n",
    "        self.tokenizer = torchtext.data.utils.get_tokenizer(\n",
    "            tokenizer, tok_lang)\n",
    "        self.unk = 0  # <unk> counter\n",
    "        self.unk_set = set()\n",
    "\n",
    "    def to_tokens(self, sentence: str):\n",
    "        return self.tokenizer(sentence)\n",
    "    \n",
    "    def to_sentence(self, indexes: list) -> str:\n",
    "        'Creates sentence from indexes. REMOVES SPECIAL TOKENS'\n",
    "        assert voc is not None, 'Method unavailable for dummy tokenizers'\n",
    "        \n",
    "        sentence = ' '.join(\n",
    "            [self.voc.itos[idx] for idx in indexes if voc.itos[idx] not in ['<sos>', '<eos>', '<pad>']])\n",
    "            \n",
    "        return sentence\n",
    "    \n",
    "    @fts.singledispatchmethod\n",
    "    def to_indexes(self, sentence, maxlen) -> list:\n",
    "        'Creates indexes from words. ADS SPECIAL TOKENS. Accepts list or str.'\n",
    "\n",
    "        raise NotImplementedError('Sentence must be a list or a str type')\n",
    "\n",
    "    @to_indexes.register  # handling sentence as str type (untokenized)\n",
    "    def _(self, sentence: str, maxlen: int):\n",
    "        assert voc is not None, 'Method unavailable for dummy tokenizers'\n",
    "\n",
    "        tokenized = deque(self.to_tokens(sentence))\n",
    "        tokenized.appendleft('<sos>')\n",
    "        tokenized.append('<eos>')\n",
    "        # padding takes into account length increase by special tokens\n",
    "        tokenized.extend(['<pad>' for _ in range(maxlen + 2 - len(tokenized))])\n",
    "\n",
    "        indexes = [self.voc.stoi.get(wrd, self.voc.stoi['<unk>']) for wrd in tokenized]\n",
    "        \n",
    "        unk_count = indexes.count(self.voc.stoi['<unk>'])\n",
    "        if unk_count:\n",
    "            self.unk += unk_count\n",
    "            self.unk_set.add(tokenized[indexes.index(self.voc.stoi['<unk>'])])\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    @to_indexes.register  # handling sentence as listy type (tokenized)\n",
    "    def _(self, sentence: list, maxlen: int):\n",
    "        assert voc is not None, 'Method unavailable for dummy tokenizers'\n",
    "        \n",
    "        sentence = map(lambda x: x.casefold(), sentence)\n",
    "        \n",
    "        sentence = deque(sentence)\n",
    "        sentence.appendleft('<sos>')\n",
    "        sentence.append('<eos>')\n",
    "        # padding takes into account length increase by special tokens\n",
    "        sentence.extend(['<pad>' for _ in range(maxlen + 2 - len(sentence))])\n",
    "\n",
    "        indexes = [self.voc.stoi.get(wrd, self.voc.stoi['<unk>']) for wrd in sentence]\n",
    "        \n",
    "        unk_count = indexes.count(self.voc.stoi['<unk>'])\n",
    "        if unk_count:\n",
    "            self.unk += unk_count\n",
    "            self.unk_set.add(sentence[indexes.index(self.voc.stoi['<unk>'])])\n",
    "\n",
    "        return indexes\n",
    "\n",
    "@fts.singledispatch\n",
    "def trimm_len(indexes, lang: Lang, voc: torchtext.vocab.Vocab, max_len_allowed: int):\n",
    "    'Trimms list of indexes to the length specified'\n",
    "    \n",
    "    raise NotImplementedError('Sentence must be a list or a str type')\n",
    "\n",
    "@trimm_len.register\n",
    "def _(indexes: list, lang: Lang, voc: torchtext.vocab.Vocab, max_len_allowed: int):\n",
    "    warnings.warn(f'You are trimming the sentence down to length={max_len_allowed}!')\n",
    "    \n",
    "    _indexes = indexes[:max_len_allowed]\n",
    "    \n",
    "    if _indexes[-1] not in [voc.stoi['<pad>'], voc.stoi['<eos>']]:\n",
    "        _indexes.append(voc.stoi['<eos>'])\n",
    "    else:\n",
    "        _indexes.append(voc.stoi['<pad>'])\n",
    "\n",
    "    return _indexes\n",
    "\n",
    "@trimm_len.register\n",
    "def _(indexes: list, lang: Lang, voc: type(None), max_len_allowed: int):\n",
    "    warnings.warn(f'You are trimming the sentence down to length={max_len_allowed}!')\n",
    "    \n",
    "    _indexes = indexes[:max_len_allowed]\n",
    "    \n",
    "    return _indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e9eee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-12T22:37:00.480571Z",
     "start_time": "2021-06-12T22:36:53.708223Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6069c019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:02.539885Z",
     "start_time": "2021-07-17T09:28:02.529903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 7, 8, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks :)\n",
    "\n",
    "txt = 'i like trains'\n",
    "txt2 = 'DuPa Cycki'\n",
    "lang = Lang('test')\n",
    "tok = Tokenizer() # dummy tokenizer\n",
    "lang.add_sentence(tok.to_tokens(txt))\n",
    "lang.add_sentence(tok.to_tokens(txt2))\n",
    "fq = Counter(lang.word2count)\n",
    "voc = Vocab(fq, min_freq=1, specials=['<sos>', '<eos>', '<pad>', '<unk>'])\n",
    "toke = Tokenizer(voc)\n",
    "toke.to_indexes(txt, lang.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b57d16a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:02.549947Z",
     "start_time": "2021-07-17T09:28:02.542823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f9648f876d0>>,\n",
       "            {'<sos>': 0,\n",
       "             '<eos>': 1,\n",
       "             '<pad>': 2,\n",
       "             '<unk>': 3,\n",
       "             'cycki': 4,\n",
       "             'dupa': 5,\n",
       "             'i': 6,\n",
       "             'like': 7,\n",
       "             'trains': 8})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82584aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:17.284430Z",
     "start_time": "2021-07-17T09:28:02.552041Z"
    }
   },
   "outputs": [],
   "source": [
    "@fts.singledispatch\n",
    "def txt_processor(sentence, lang: Lang) -> list:\n",
    "    'Preprosess sentence and load data to Lang class instance'\n",
    "    \n",
    "    raise NotImplementedError('Sentence must be a list or a str type')\n",
    "\n",
    "@txt_processor.register\n",
    "def _(sentence: str, lang: Lang) -> list:\n",
    "    _sentence = Tokenizer().to_tokens(sentence)\n",
    "    lang.add_sentence(_sentence)\n",
    "    \n",
    "    return _sentence\n",
    "\n",
    "@txt_processor.register\n",
    "def _(sentence: list, lang: Lang) -> list:\n",
    "    _sentence = [wrd.casefold() for wrd in sentence]\n",
    "    lang.add_sentence(_sentence)\n",
    "    \n",
    "    return _sentence\n",
    "        \n",
    "lang_data = Lang('metadata_for_flickr_captions')   \n",
    "captions = df_source.apply(lambda x: txt_processor(x['caption'], lang_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11607ff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:17.772878Z",
     "start_time": "2021-07-17T09:28:17.285934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang_data.plt_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b16af8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:18.416329Z",
     "start_time": "2021-07-17T09:28:17.774376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-34ea7a530965>:132: UserWarning: You are trimming the sentence down to length=20!\n",
      "  warnings.warn(f'You are trimming the sentence down to length={max_len_allowed}!')\n"
     ]
    }
   ],
   "source": [
    "# as we can see, we have quite long tail on the right\n",
    "# computation will be faster if we trimm out e.g. top 5 % the longest sentences\n",
    "max_len_allowed = int(np.percentile(lang_data.stats, 90, interpolation='nearest'))\n",
    "captions_shorter = captions.apply(lambda x: trimm_len(x, lang_data, None, max_len_allowed))\n",
    "df_source_shorter = df_source.copy()\n",
    "df_source_shorter['caption'] = captions_shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "701798f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:32.268843Z",
     "start_time": "2021-07-17T09:28:18.417588Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_data_shorter = Lang('metadata_for_flickr_captions_shorter')   \n",
    "_ = df_source_shorter.apply(lambda x: txt_processor(x['caption'], lang_data_shorter), axis=1)\n",
    "frequency = Counter(lang_data_shorter.word2count)\n",
    "vocabulary = Vocab(frequency, min_freq=10, vectors='glove.6B.200d',\n",
    "                   specials=['<sos>', '<eos>', '<pad>', '<unk>']) # , unk_init=torch.Tensor.normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7342164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:35.080182Z",
     "start_time": "2021-07-17T09:28:32.270769Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocabulary)\n",
    "tokenized_captions = captions_shorter.apply(lambda x: tokenizer.to_indexes(x, lang_data_shorter.maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145f4c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:35.330949Z",
     "start_time": "2021-07-17T09:28:35.081662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lang_data_shorter.plt_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb6e2090",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:35.337004Z",
     "start_time": "2021-07-17T09:28:35.332590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4681346744971693"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of <unk>\n",
    "(tokenizer.unk/sum(lang_data_shorter.word2count.values())) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18fff621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:35.357057Z",
     "start_time": "2021-07-17T09:28:35.338950Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tokenized = df_source_shorter.copy()\n",
    "df_tokenized['caption'] = tokenized_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73951332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.373639Z",
     "start_time": "2021-07-17T09:28:35.360277Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_sampler(s: set):\n",
    "    elem = random.sample(s, 1)[0]\n",
    "    s.remove(elem)\n",
    "    \n",
    "    return elem\n",
    "\n",
    "pic_set = set(df_tokenized['image'])\n",
    "unique_pic_val = [set_sampler(pic_set) for _ in range((256*8)//5)]\n",
    "unique_pic_test = [set_sampler(pic_set) for _ in range((256*16)//5)]\n",
    "df_val = df_tokenized[df_tokenized['image'].isin(unique_pic_val)]\n",
    "df_test = df_tokenized[df_tokenized['image'].isin(unique_pic_test)]\n",
    "df_train = df_tokenized.drop(df_val.index).drop(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fe343b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.382806Z",
     "start_time": "2021-07-17T09:28:36.375509Z"
    }
   },
   "outputs": [],
   "source": [
    "del df_source\n",
    "del df_source_shorter\n",
    "del df_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7030287f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.445767Z",
     "start_time": "2021-07-17T09:28:36.396117Z"
    }
   },
   "outputs": [],
   "source": [
    "@fts.singledispatch\n",
    "def find_closest(wrd, vocab: torchtext.vocab.Vocab, n: int) -> list:\n",
    "    'finding n closest words as indexes'\n",
    "    \n",
    "    raise NotImplementedError(f'wrd must be an int or a str type. Is {type(wrd)}')\n",
    "\n",
    "@find_closest.register\n",
    "def _(wrd: int, vocab: torchtext.vocab.Vocab, n: int) -> list:\n",
    "    'finding n closest words as indexes'\n",
    "    vec = vocab.vectors\n",
    "    wds = sorted(vocab.stoi.values(), key=lambda x: spatial.distance.euclidean(vec[wrd], vec[x]))[1:n+1]\n",
    "    wds_str = [vocab.itos[i] for i in wds]\n",
    "    \n",
    "    if wds[0] in [0, 1, 2, 3]:\n",
    "        warnings.warn('This word probably does not exist in the embedding!')\n",
    "    \n",
    "    return wds, wds_str, vocab.itos[wrd]\n",
    "\n",
    "@find_closest.register\n",
    "def _(wrd: str, vocab: torchtext.vocab.Vocab, n: int) -> list:\n",
    "    'finding n closest words as indexes'\n",
    "    vec = vocab.vectors\n",
    "    idx = vocab.stoi[wrd]\n",
    "    wds = sorted(vocab.stoi.values(), key=lambda x: spatial.distance.euclidean(vec[idx], vec[x]))[1:n+1]\n",
    "    wds_str = [vocab.itos[i] for i in wds]\n",
    "    \n",
    "    if wds[0] in [0, 1, 2, 3]:\n",
    "        warnings.warn('This word probably does not exist in the embedding!')\n",
    "    \n",
    "    return wds, wds_str, wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d13179e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.606571Z",
     "start_time": "2021-07-17T09:28:36.594976Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining dataset loader\n",
    "\n",
    "\n",
    "class FlickrData(Dataset):\n",
    "    def __init__(self,\n",
    "                 img_names: pd.Series,\n",
    "                 sequence_in: pd.Series,\n",
    "                 pic_folder: str,\n",
    "                 channels: int,\n",
    "                 rotation=0,\n",
    "                 shear=0,\n",
    "                 translation=0,\n",
    "                 seed=0):\n",
    "        '''\n",
    "        rotation in degrees\n",
    "        shear in radians\n",
    "        translation as tuple (x, y)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.x1 = img_names.to_list()\n",
    "        self.x2 = np.array(\n",
    "            [np.array(item, dtype=np.int_) for _, item in sequence_in.iteritems()],\n",
    "            dtype=np.int_)\n",
    "        self.pic_folder = pic_folder\n",
    "        self.channels = channels\n",
    "        self.rotation = rotation\n",
    "        self.shear = shear\n",
    "        self.translation = translation\n",
    "        self.samples = len(self.x1)\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x1 = self.preprocess_img(self.x1[idx])\n",
    "        x2 = torch.tensor(self.x2[idx]).to(device).long()\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def preprocess_img(self, img_name):\n",
    "        img_path = os.path.join(self.pic_folder, img_name)\n",
    "        transormation = AffineTransform(\n",
    "            rotation=self.rng.normal(0, self.rotation),\n",
    "            shear=self.rng.normal(0, self.shear),\n",
    "            translation=(self.rng.normal(0, self.translation),\n",
    "                         self.rng.normal(0, self.translation)))\n",
    "        \n",
    "        img = np.float32(imread(img_path))\n",
    "        if self.rng.uniform() > 0.5:\n",
    "            img = np.flip(img, 1)\n",
    "        img = resize(warp(img / 255, transormation),\n",
    "                   (224, 224, self.channels))\n",
    "        img = torch.tensor(img).permute(2, 0, 1)\n",
    "        img = self.normalize(img)\n",
    "        \n",
    "        return img.to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0079d0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.619281Z",
     "start_time": "2021-07-17T09:28:36.609280Z"
    }
   },
   "outputs": [],
   "source": [
    "class SepConv2d(nn.Module):\n",
    "    'Depthwise separable version of 2D convolution'\n",
    "    def __init__(self,\n",
    "                 chan_in: int,\n",
    "                 chan_out: int,\n",
    "                 kernel_size: int,\n",
    "                 stride: int,\n",
    "                 padding: Union[int, tuple, str]):\n",
    "        super().__init__()\n",
    "        self.depth = nn.Conv2d(chan_in, chan_in, kernel_size=kernel_size,\n",
    "                               groups=chan_in, stride=stride, padding=padding)\n",
    "        self.point = nn.Conv2d(chan_in, chan_out, kernel_size=1,\n",
    "                               stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.depth(x)\n",
    "        x = self.point(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06fc4bbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.633417Z",
     "start_time": "2021-07-17T09:28:36.620852Z"
    }
   },
   "outputs": [],
   "source": [
    "# early convolutions help transformer see better https://arxiv.org/pdf/2106.14881.pdf\n",
    "class PatchEmb(nn.Module):\n",
    "    def __init__(self, pic_dims, hid_dim, dropout, device):\n",
    "        assert pic_dims[1] == pic_dims[2], 'Reshape a picture to even (h w) dims'\n",
    "        assert all([dim % 16 == 0 for dim in pic_dims[1:]]), 'Reshape a picture to dims divisable by 16'\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv = nn.Sequential(SepConv2d(pic_dims[0], pic_dims[0], kernel_size=3,\n",
    "                                    stride=1, padding=1), nn.BatchNorm2d(pic_dims[0]), nn.ReLU(),\n",
    "                                  SepConv2d(pic_dims[0], 8, kernel_size=3,\n",
    "                                    stride=2, padding=1), nn.BatchNorm2d(8), nn.ReLU(),\n",
    "                                  SepConv2d(8, 16, kernel_size=3,\n",
    "                                    stride=2, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "                                  SepConv2d(16, 32, kernel_size=3,\n",
    "                                    stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "                                  SepConv2d(32, 64, kernel_size=3,\n",
    "                                    stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU())\n",
    "        self.emb = nn.Sequential(SepConv2d(64, hid_dim, kernel_size=1,\n",
    "                                    stride=1, padding=0), nn.Flatten(-2, -1))\n",
    "        self.synth_len = (pic_dims[1]//16)**2 + 1\n",
    "        self.pos_embedding = nn.Embedding(self.synth_len, hid_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hid_dim))\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, pic):\n",
    "        \n",
    "        pos = torch.arange(0, self.synth_len\n",
    "                          ).unsqueeze(0).repeat(pic.shape[0], 1).to(self.device)\n",
    "        cls_tok = self.cls_token.repeat(pic.shape[0], 1, 1).to(self.device)\n",
    "        x = self.conv(pic)\n",
    "        x = self.emb(x).permute(0, 2, 1)\n",
    "        x = torch.cat([cls_tok, x], dim=1)\n",
    "        x += self.pos_embedding(pos)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d327aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.644777Z",
     "start_time": "2021-07-17T09:28:36.635591Z"
    }
   },
   "outputs": [],
   "source": [
    "class PicEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pic_dims,\n",
    "                 n_layers,\n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        self.embedder = PatchEmb(pic_dims, hid_dim, dropout, device)\n",
    "        self.transformer = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                       n_heads, \n",
    "                                                       pf_dim, \n",
    "                                                       dropout, \n",
    "                                                       device) for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedder(x)\n",
    "        \n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9647ce73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.653501Z",
     "start_time": "2021-07-17T09:28:36.646696Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(hid_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiQueryAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                       pf_dim, \n",
    "                                                                       dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        \n",
    "        src = self.layer_norm1(src + self.dropout(_src))\n",
    "\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.layer_norm2(src + self.dropout(_src))\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6696f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.666951Z",
     "start_time": "2021-07-17T09:28:36.656075Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiQueryAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads,  dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.hid_dim // self.n_heads\n",
    "\n",
    "        self.Pq = nn.Linear(self.hid_dim, self.hid_dim)\n",
    "        self.Pk = nn.Linear(self.hid_dim, self.head_dim)\n",
    "        self.Pv = nn.Linear(self.hid_dim, self.head_dim)\n",
    "        self.Po = nn.Linear(self.hid_dim, self.hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        query_len = query.shape[1]\n",
    "        key_len = key.shape[1]\n",
    "        value_len = value.shape[1]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "\n",
    "        Qbank = self.Pq(query).view(batch_size, -1, self.n_heads, self.head_dim)\n",
    "        Kbank = self.Pk(key).view(batch_size, -1, self.head_dim)\n",
    "        Vbank = self.Pv(value).view(batch_size, -1, self.head_dim)\n",
    "        \n",
    "        #Qbank = [batch size, query len, n heads, head dim]\n",
    "        #Kbank = [batch size, key len, head dim]\n",
    "        #Vbank = [batch size, value len, head dim ]\n",
    "\n",
    "        energy = torch.einsum('bmhd,bnd->bhmn',Qbank,Kbank) / self.scale\n",
    "\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        attention = F.softmax(energy, dim = -1)\n",
    "                \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "        o = torch.einsum('bhmn,bnv->bmhv',self.dropout(attention), Vbank).contiguous()\n",
    "        o = o.view(batch_size,query_len,-1)\n",
    "\n",
    "        y = self.Po(o)\n",
    "        \n",
    "        #y = [batch size, query len, hid dim]\n",
    "        \n",
    "        return y, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0075cdeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.680474Z",
     "start_time": "2021-07-17T09:28:36.668532Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(F.gelu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce8131de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.693481Z",
     "start_time": "2021-07-17T09:28:36.682100Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 embedding,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length,\n",
    "                 pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # pretrained embedding\n",
    "        self.tok_embedding = nn.Parameter(embedding, requires_grad=False)\n",
    "        # self.tok_embedding = nn.Embedding(output_dim, hid_dim, padding_idx=pad_idx)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask):  # src_mask\n",
    "\n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size,\n",
    "                                                           1).to(self.device)\n",
    "\n",
    "        #pos = [batch size, trg len]\n",
    "        trg = self.dropout((self.tok_embedding[trg]  # accesing non nn embedding should be with []\n",
    "                            * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask)  # src_mask\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        #output = [batch size, trg len, output dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d8f5a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.707384Z",
     "start_time": "2021-07-17T09:28:36.695116Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(hid_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hid_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiQueryAttentionLayer(\n",
    "            hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiQueryAttentionLayer(\n",
    "            hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(\n",
    "            hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask):   # src_mask\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "\n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm1(trg + self.dropout(_trg))\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src)  # src_mask removed due to encoder nature\n",
    "\n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm2(trg + self.dropout(_trg))\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.layer_norm3(trg + self.dropout(_trg))\n",
    "\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "914966ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:36.720877Z",
     "start_time": "2021-07-17T09:28:36.709787Z"
    }
   },
   "outputs": [],
   "source": [
    "class Pic2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        #trg = [batch size, trg len]\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "        #trg_pad_mask = [batch size, 1, trg len, 1]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(\n",
    "            torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "\n",
    "        trg_mask = (trg_pad_mask & trg_sub_mask)\n",
    "\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        #src1 = [batch_size, channels, dimy, dimx]\n",
    "        #src2 = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        # src_mask = self.make_src_mask(src2)\n",
    "        # src_mask = torch.ones(enc_src.shape[:-1])  # just a filler mask\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        enc_src = self.encoder(src)\n",
    "\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask) \n",
    "\n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624a488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-26T18:49:02.298014Z",
     "start_time": "2021-06-26T18:49:02.291818Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43c01d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:39.363543Z",
     "start_time": "2021-07-17T09:28:36.723081Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIM = len(vocabulary.stoi.keys())\n",
    "HID_DIM = 200 # 300\n",
    "PIC_DIMS = (3, 224, 224)\n",
    "ENC_LAYERS = 7 # 11   # 5 convolutions in patch embedding does some work\n",
    "DEC_LAYERS = 8 # 12\n",
    "HEADS = 5\n",
    "PF_DIM = 512  # if > HID_DIM FF layer behaves as MLP, if < HID_DIM FF layer behaves as AE\n",
    "# picture augumentation and many captions for one picture have regularizing effect also\n",
    "ENC_DROPOUT = 0.2  \n",
    "DEC_DROPOUT = 0.1\n",
    "MAXLEN = len(tokenized_captions.iloc[0])\n",
    "PAD_IDX = vocabulary.stoi['<pad>']\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "enc = PicEncoder(PIC_DIMS,\n",
    "              ENC_LAYERS,\n",
    "              HID_DIM, \n",
    "              HEADS, \n",
    "              PF_DIM,\n",
    "              ENC_DROPOUT,\n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              HEADS, \n",
    "              PF_DIM, \n",
    "              vocabulary.vectors.to(device).float(),\n",
    "              DEC_DROPOUT, \n",
    "              device,\n",
    "              MAXLEN,\n",
    "              PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d12237a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:39.810687Z",
     "start_time": "2021-07-17T09:28:39.365349Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = FlickrData(img_names=df_train.loc[:, 'image'],\n",
    "                        sequence_in=df_train.loc[:, 'caption'],\n",
    "                        pic_folder=pic_dir,\n",
    "                        channels=3,\n",
    "                        rotation=np.deg2rad(15),\n",
    "                        shear=np.deg2rad(15),\n",
    "                        translation=15,\n",
    "                        seed=SEED)\n",
    "\n",
    "val_data = FlickrData(img_names=df_val.loc[:, 'image'],\n",
    "                      sequence_in=df_val.loc[:, 'caption'],\n",
    "                      pic_folder=pic_dir,\n",
    "                      channels=3,\n",
    "                      seed=SEED)\n",
    "\n",
    "test_data = FlickrData(img_names=df_test.loc[:, 'image'],\n",
    "                      sequence_in=df_test.loc[:, 'caption'],\n",
    "                      pic_folder=pic_dir,\n",
    "                      channels=3,\n",
    "                      seed=SEED)\n",
    "trainflow = DataLoader(train_data, BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "valflow = DataLoader(val_data, BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "testflow = DataLoader(test_data, 1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61d35005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:39.816517Z",
     "start_time": "2021-07-17T09:28:39.812195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Train batches: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>. Val batches: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>. Test batches: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4095</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Train batches: \u001b[1;36m1194\u001b[0m. Val batches: \u001b[1;36m16\u001b[0m. Test batches: \u001b[1;36m4095\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Train batches: {len(trainflow)}. Val batches: {len(valflow)}. Test batches: {len(testflow)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "791a3d16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:39.846843Z",
     "start_time": "2021-07-17T09:28:39.818041Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = Pic2Seq(enc, dec, PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c3b7887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:40.500550Z",
     "start_time": "2021-07-17T09:28:39.848171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "Layer (type:depth-idx)                                       Output Shape              Param #\n",
      "==============================================================================================================\n",
      "├─PicEncoder: 1-1                                            [-1, 197, 200]            --\n",
      "|    └─PatchEmb: 2-1                                         [-1, 197, 200]            --\n",
      "|    |    └─Sequential: 3-1                                  [-1, 64, 14, 14]          3,710\n",
      "|    |    └─Sequential: 3-2                                  [-1, 200, 196]            13,128\n",
      "|    |    └─Embedding: 3-3                                   [-1, 197, 200]            39,400\n",
      "|    └─ModuleList: 2                                         []                        --\n",
      "|    |    └─EncoderLayer: 3-4                                [-1, 197, 200]            302,792\n",
      "|    |    └─EncoderLayer: 3-5                                [-1, 197, 200]            302,792\n",
      "|    |    └─EncoderLayer: 3-6                                [-1, 197, 200]            302,792\n",
      "|    |    └─EncoderLayer: 3-7                                [-1, 197, 200]            302,792\n",
      "|    |    └─EncoderLayer: 3-8                                [-1, 197, 200]            302,792\n",
      "|    |    └─EncoderLayer: 3-9                                [-1, 197, 200]            302,792\n",
      "|    |    └─EncoderLayer: 3-10                               [-1, 197, 200]            302,792\n",
      "├─Decoder: 1-2                                               [-1, 22, 5328]            --\n",
      "|    └─Embedding: 2-2                                        [-1, 22, 200]             4,400\n",
      "|    └─Dropout: 2-3                                          [-1, 22, 200]             --\n",
      "|    └─ModuleList: 2                                         []                        --\n",
      "|    |    └─DecoderLayer: 3-11                               [-1, 22, 200]             399,672\n",
      "|    |    └─DecoderLayer: 3-12                               [-1, 22, 200]             399,672\n",
      "|    |    └─DecoderLayer: 3-13                               [-1, 22, 200]             399,672\n",
      "|    |    └─DecoderLayer: 3-14                               [-1, 22, 200]             399,672\n",
      "|    |    └─DecoderLayer: 3-15                               [-1, 22, 200]             399,672\n",
      "|    |    └─DecoderLayer: 3-16                               [-1, 22, 200]             399,672\n",
      "|    |    └─DecoderLayer: 3-17                               [-1, 22, 200]             399,672\n",
      "|    |    └─DecoderLayer: 3-18                               [-1, 22, 200]             399,672\n",
      "|    └─Linear: 2-4                                           [-1, 22, 5328]            1,070,928\n",
      "==============================================================================================================\n",
      "Total params: 6,448,486\n",
      "Trainable params: 6,448,486\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 18.19\n",
      "==============================================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 8.83\n",
      "Params size (MB): 24.60\n",
      "Estimated Total Size (MB): 34.00\n",
      "==============================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "├─PicEncoder: 1-1                                            [-1, 197, 200]            --\n",
       "|    └─PatchEmb: 2-1                                         [-1, 197, 200]            --\n",
       "|    |    └─Sequential: 3-1                                  [-1, 64, 14, 14]          3,710\n",
       "|    |    └─Sequential: 3-2                                  [-1, 200, 196]            13,128\n",
       "|    |    └─Embedding: 3-3                                   [-1, 197, 200]            39,400\n",
       "|    └─ModuleList: 2                                         []                        --\n",
       "|    |    └─EncoderLayer: 3-4                                [-1, 197, 200]            302,792\n",
       "|    |    └─EncoderLayer: 3-5                                [-1, 197, 200]            302,792\n",
       "|    |    └─EncoderLayer: 3-6                                [-1, 197, 200]            302,792\n",
       "|    |    └─EncoderLayer: 3-7                                [-1, 197, 200]            302,792\n",
       "|    |    └─EncoderLayer: 3-8                                [-1, 197, 200]            302,792\n",
       "|    |    └─EncoderLayer: 3-9                                [-1, 197, 200]            302,792\n",
       "|    |    └─EncoderLayer: 3-10                               [-1, 197, 200]            302,792\n",
       "├─Decoder: 1-2                                               [-1, 22, 5328]            --\n",
       "|    └─Embedding: 2-2                                        [-1, 22, 200]             4,400\n",
       "|    └─Dropout: 2-3                                          [-1, 22, 200]             --\n",
       "|    └─ModuleList: 2                                         []                        --\n",
       "|    |    └─DecoderLayer: 3-11                               [-1, 22, 200]             399,672\n",
       "|    |    └─DecoderLayer: 3-12                               [-1, 22, 200]             399,672\n",
       "|    |    └─DecoderLayer: 3-13                               [-1, 22, 200]             399,672\n",
       "|    |    └─DecoderLayer: 3-14                               [-1, 22, 200]             399,672\n",
       "|    |    └─DecoderLayer: 3-15                               [-1, 22, 200]             399,672\n",
       "|    |    └─DecoderLayer: 3-16                               [-1, 22, 200]             399,672\n",
       "|    |    └─DecoderLayer: 3-17                               [-1, 22, 200]             399,672\n",
       "|    |    └─DecoderLayer: 3-18                               [-1, 22, 200]             399,672\n",
       "|    └─Linear: 2-4                                           [-1, 22, 5328]            1,070,928\n",
       "==============================================================================================================\n",
       "Total params: 6,448,486\n",
       "Trainable params: 6,448,486\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 18.19\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 8.83\n",
       "Params size (MB): 24.60\n",
       "Estimated Total Size (MB): 34.00\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model summary\n",
    "pic_in = torch.randn(1, 3, 224, 224).float()\n",
    "seq_in = torch.randn(1, MAXLEN).long()\n",
    "summary(model, pic_in, seq_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08144cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:40.505049Z",
     "start_time": "2021-07-17T09:28:40.502408Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5e0846b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:40.513880Z",
     "start_time": "2021-07-17T09:28:40.506727Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6194602d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:40.523884Z",
     "start_time": "2021-07-17T09:28:40.515797Z"
    }
   },
   "outputs": [],
   "source": [
    "'''MAX_RATE = 0.01\n",
    "MIN_RATE = 0.00001'''\n",
    "\n",
    "MAX_RATE = 0.2\n",
    "MIN_RATE = 0.00001\n",
    "\n",
    "# papers as https://arxiv.org/pdf/2004.08249.pdf claims SGD is bad at transformers\n",
    "# BUT I am using scheduler, so it is different situation\n",
    "# SGD with schedule is far more computation efficient than eg. Adam\n",
    "# additionaly, low MIN_RATE acts as warmup stage, and later may help with local minima\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=MIN_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n",
    "                                              base_lr=MIN_RATE,\n",
    "                                              max_lr=MAX_RATE,\n",
    "                                              mode='triangular',\n",
    "                                              step_size_up=len(trainflow) * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "520229bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:40.547786Z",
     "start_time": "2021-07-17T09:28:40.537030Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = vocabulary.stoi['<pad>']) # ignore PAD index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1da75e0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:40.556423Z",
     "start_time": "2021-07-17T09:28:40.549308Z"
    }
   },
   "outputs": [],
   "source": [
    "ckp_log = os.path.join(ckp_dir, f'last_ckp_{savename}.csv')\n",
    "\n",
    "if not os.path.exists(ckp_log):\n",
    "    Path(ckp_log).touch()\n",
    "    with open(ckp_log, 'a') as file:\n",
    "        line = ','\n",
    "        file.write(line)\n",
    "\n",
    "# last_cpk = pd.read_csv(ckp_log).iloc[-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b93685dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:40.570982Z",
     "start_time": "2021-07-17T09:28:40.557720Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(src, trg, model, opt, crit, scaler, scheduler):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    with autocast():\n",
    "        pred_seq, _ = model(src, trg) # model(src, trg[:,:-1])\n",
    "        pad = np.array([vocabulary.stoi['<pad>']])\n",
    "        pad = np.expand_dims(pad, axis=0)\n",
    "        pad = np.repeat(pad, trg.shape[0], axis=0) # batch repeat\n",
    "        pad = torch.LongTensor(pad).to(device)\n",
    "        trg = torch.cat([trg, pad], dim=-1)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        pred_seq = pred_seq.contiguous().view(-1, pred_seq.shape[-1])\n",
    "        loss = crit(pred_seq, trg)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(opt)  # for clipping\n",
    "    torch.nn.utils.clip_grad_value_(model.parameters(), 1.0)\n",
    "    scaler.step(opt)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "    return loss, pred_seq\n",
    "\n",
    "def val_batch(src, trg, model, crit):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            pred_seq, _ = model(src, trg)\n",
    "            pad = np.array([vocabulary.stoi['<pad>']])\n",
    "            pad = np.expand_dims(pad, axis=0)\n",
    "            pad = np.repeat(pad, trg.shape[0], axis=0) # batch repeat\n",
    "            pad = torch.LongTensor(pad).to(device)\n",
    "            trg = torch.cat([trg, pad], dim=-1)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            pred_seq = pred_seq.contiguous().view(-1, pred_seq.shape[-1])\n",
    "            loss = crit(pred_seq, trg)\n",
    "    return loss, pred_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "008ffde3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:28:40.589270Z",
     "start_time": "2021-07-17T09:28:40.572143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pic2Seq(\n",
       "  (encoder): PicEncoder(\n",
       "    (embedder): PatchEmb(\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (conv): Sequential(\n",
       "        (0): SepConv2d(\n",
       "          (depth): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3)\n",
       "          (point): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): SepConv2d(\n",
       "          (depth): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=3)\n",
       "          (point): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "        (6): SepConv2d(\n",
       "          (depth): Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8)\n",
       "          (point): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (8): ReLU()\n",
       "        (9): SepConv2d(\n",
       "          (depth): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16)\n",
       "          (point): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (11): ReLU()\n",
       "        (12): SepConv2d(\n",
       "          (depth): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)\n",
       "          (point): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (14): ReLU()\n",
       "      )\n",
       "      (emb): Sequential(\n",
       "        (0): SepConv2d(\n",
       "          (depth): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), groups=64)\n",
       "          (point): Conv2d(64, 200, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Flatten(start_dim=-2, end_dim=-1)\n",
       "      )\n",
       "      (pos_embedding): Embedding(197, 200)\n",
       "    )\n",
       "    (transformer): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (6): EncoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (pos_embedding): Embedding(22, 200)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (layer_norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiQueryAttentionLayer(\n",
       "          (Pq): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (Pk): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Pv): Linear(in_features=200, out_features=40, bias=True)\n",
       "          (Po): Linear(in_features=200, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=200, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=200, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=200, out_features=5328, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = GradScaler()  # prevents gradient underflow in f16\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b1f28",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-17T09:27:58.736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1.000\ttrn_loss: 5.224\tval_loss: 4.227\t(4331.58s - 134278.95s remaining)\n",
      "EPOCH: 2.000\ttrn_loss: 3.890\tval_loss: 3.599\t(8718.00s - 130770.04s remaining)\n",
      "EPOCH: 3.000\ttrn_loss: 3.472\tval_loss: 3.328\t(13004.77s - 125712.81s remaining)\n",
      "EPOCH: 4.000\ttrn_loss: 3.284\tval_loss: 3.210\t(17267.38s - 120871.66s remaining)\n",
      "EPOCH: 5.000\ttrn_loss: 3.162\tval_loss: 3.105\t(21578.24s - 116522.47s remaining)\n",
      "EPOCH: 6.000\ttrn_loss: 3.070\tval_loss: 3.033\t(25936.52s - 112391.59s remaining)\n",
      "EPOCH: 7.000\ttrn_loss: 3.003\tval_loss: 2.985\t(30352.24s - 108400.85s remaining)\n",
      "EPOCH: 8.000\ttrn_loss: 2.942\tval_loss: 2.948\t(34722.20s - 104166.61s remaining)\n",
      "EPOCH: 9.000\ttrn_loss: 2.924\tval_loss: 2.961\t(38897.05s - 99403.57s remaining)\n",
      "EPOCH: 10.000\ttrn_loss: 2.947\tval_loss: 2.972\t(43044.88s - 94698.73s remaining)\n",
      "EPOCH: 11.000\ttrn_loss: 2.949\tval_loss: 2.961\t(47174.87s - 90061.11s remaining)\n",
      "EPOCH: 12.000\ttrn_loss: 2.938\tval_loss: 2.963\t(51306.74s - 85511.24s remaining)\n",
      "EPOCH: 13.000\ttrn_loss: 2.910\tval_loss: 2.935\t(55427.43s - 81009.32s remaining)\n",
      "EPOCH: 14.000\ttrn_loss: 2.868\tval_loss: 2.907\t(59550.16s - 76564.50s remaining)\n",
      "EPOCH: 15.000\ttrn_loss: 2.825\tval_loss: 2.886\t(63655.13s - 72142.48s remaining)\n",
      "EPOCH: 16.000\ttrn_loss: 2.777\tval_loss: 2.858\t(67772.50s - 67772.50s remaining)\n",
      "EPOCH: 17.000\ttrn_loss: 2.763\tval_loss: 2.871\t(71881.09s - 63424.50s remaining)\n",
      "EPOCH: 18.000\ttrn_loss: 2.796\tval_loss: 2.900\t(76005.41s - 59115.32s remaining)\n",
      "EPOCH: 19.000\ttrn_loss: 2.812\tval_loss: 2.907\t(80143.81s - 54835.24s remaining)\n",
      "EPOCH: 19.593\ttrn_loss: 2.856\t(82786.83s - 52423.88s remaining)"
     ]
    }
   ],
   "source": [
    "epochs = 32\n",
    "logger = Report(epochs)\n",
    "for epoch in range(epochs):\n",
    "        \n",
    "    N = len(trainflow)\n",
    "    for ix, srctrg in enumerate(trainflow):\n",
    "        src, trg = srctrg\n",
    "        total_loss, pred_seq = train_batch(src, trg, model, optimizer,\n",
    "                                           criterion, scaler, scheduler)\n",
    "        logger.record(epoch + (ix + 1) / N, trn_loss=total_loss, end='\\r')\n",
    "\n",
    "    \n",
    "    if epoch % 8 == 0:\n",
    "        cpk = f'cpk_{savename}_{epoch}.tar'\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': total_loss\n",
    "            }, os.path.join(ckp_dir, cpk))\n",
    "                \n",
    "        with open(os.path.join(ckp_dir, cpk), 'a') as file:\n",
    "            line = [cpk, time.time()]\n",
    "            writer = csv.writer(file, delimiter=' ')\n",
    "            writer.writerow(line)\n",
    "                \n",
    "    N = len(valflow)\n",
    "    for ix, srctrg in enumerate(valflow):\n",
    "        src, trg = srctrg\n",
    "        total_loss, pred_seq = val_batch(src, trg, model, criterion)\n",
    "        logger.record(epoch + (ix + 1) / N, val_loss=total_loss, end='\\r')\n",
    "                \n",
    "    logger.report_avgs(epoch+1)\n",
    "                \n",
    "logger.plot_epochs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce0ee4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-17T09:27:58.738Z"
    }
   },
   "outputs": [],
   "source": [
    "N = len(testflow)\n",
    "losses = []\n",
    "seqs = []\n",
    "for ix, srctrg in tqdm(enumerate(testflow)):\n",
    "    src, trg = srctrg\n",
    "    total_loss, pred_seq = val_batch(src, trg, model, criterion)\n",
    "    losses.append(total_loss)\n",
    "    seqs.append((trg.cpu().numpy(), pred_seq.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33f1f3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-17T09:27:58.740Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Avg loss: {sum(losses)/len(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d3558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T05:38:23.760268Z",
     "start_time": "2021-07-16T05:38:23.368914Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(ckp_dir, 'cpk_pic2sec_pytorch_vit_8k_glove_convemb_10.tar'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b441b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T10:42:56.995151Z",
     "start_time": "2021-07-15T10:42:56.979979Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer_single(src, trg, model):\n",
    "    src_tensor = src # torch.FloatTensor(src).unsqueeze(0).to(device)\n",
    "    trg_tensor = torch.LongTensor(trg).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            enc_src = model.encoder(src_tensor)\n",
    "            trg_mask = model.make_trg_mask(trg_tensor)\n",
    "            pred, _ = model.decoder(trg_tensor, enc_src, trg_mask)\n",
    "            pred = nn.LogSoftmax(dim=2)(pred)\n",
    "            pred_argsort = pred.argsort(2)\n",
    "            pred_best = pred_argsort[:,-1, -1].item()\n",
    "            pred_second_best = pred_argsort[:,-1, -2].item()\n",
    "            score_best = pred[:, -1, pred_best].item()\n",
    "            score_second_best = pred[:, -1, pred_second_best].item()\n",
    "    \n",
    "    # returns predicted indexes, suitable for simple beam search\n",
    "    return (pred_best, score_best), (pred_second_best, score_second_best)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
